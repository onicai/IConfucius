{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IConfucius 提示设计用于 llama_cpp_canister\n",
    "\n",
    "## 中文版本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 设置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证我们是否在 Conda 环境中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入 Python 包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import base64\n",
    "import io\n",
    "\n",
    "# from dotenv import load_dotenv\n",
    "# import requests\n",
    "import pprint\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import jupyter_black\n",
    "import textwrap\n",
    "\n",
    "import random\n",
    "\n",
    "from run_llama_cpp import run_llama_cpp\n",
    "\n",
    "# Activate the jupyter_black extension, which reformats code cells with black\n",
    "# https://github.com/n8henrie/jupyter-black\n",
    "jupyter_black.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################################################################\n",
    "# 先构建常规的 llama.cpp，然后指定其路径 \n",
    "# \n",
    "# 要构建 llama.cpp，请按照 README 中的说明进行操作：\n",
    "# https://github.com/ggml-org/llama.cpp  \n",
    "#    \n",
    "# 定义 llama-cli 的位置，相对于此笔记本的路径  \n",
    "# ################################################################\n",
    "\n",
    "\n",
    "# 使用最新版本的 llama.cpp\n",
    "# LLAMA_CLI_PATH = \"../../ggml_org_llama_latest.cpp/build/bin/llama-cli\"\n",
    "\n",
    "# 当前版本的 llama_cpp_canister 使用的是 llama.cpp Git SHA 615212\n",
    "LLAMA_CLI_PATH = \"../../ggml_org_llama_615212.cpp/build/bin/llama-cli\"\n",
    "\n",
    "\n",
    "# #######################################################################\n",
    "# 选择 MODEL_TYPE 和 MODEL（位置相对于此笔记本）\n",
    "# #######################################################################\n",
    "\n",
    "seed = random.randint(0, 10000000)\n",
    "num_tokens = 1024\n",
    "temp = 0.7\n",
    "# top_k = 50\n",
    "# top_p = 0.95\n",
    "# min_p = 0.05\n",
    "# tfs = 0.9\n",
    "# typical = 0.9\n",
    "# mirostat = 2\n",
    "# mirostat_lr = 0.1\n",
    "# mirostat_ent = 5.0\n",
    "repeat_penalty = 1.1\n",
    "\n",
    "# Notes:\n",
    "#                                     <not quantized>|<         quantized                >\n",
    "#  --cache-type-k has allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1\n",
    "#  --cache-type-v is not tested because that requires a GPU,\n",
    "#                 which is not available right now in an Internet Computer canister\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# 163 Million parameters\n",
    "\n",
    "# https://huggingface.co/tensorblock/gpt2-GGUF (124M)\n",
    "# MODEL_TYPE = \"gpt2\"\n",
    "# MODEL = \"../llms/llama_cpp_canister/models/tensorblock/gpt2-GGUF/gpt2-Q8_0.gguf\"\n",
    "# cache_type_k = \"f16\"\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# 630 Million parameters\n",
    "\n",
    "# https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GGUF\n",
    "# MODEL_TYPE = \"Qwen\"\n",
    "# MODEL = \"../llms/llama_cpp_canister/models/Qwen/Qwen2.5-0.5B-Instruct-GGUF/qwen2.5-0.5b-instruct-q4_k_m.gguf\"\n",
    "# cache_type_k = \"f16\"\n",
    "\n",
    "MODEL_TYPE = \"Qwen\"\n",
    "MODEL = \"../llms/llama_cpp_canister/models/Qwen/Qwen2.5-0.5B-Instruct-GGUF/qwen2.5-0.5b-instruct-q8_0.gguf\"\n",
    "cache_type_k = \"q8_0\"\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# 1.24 Billion parameters\n",
    "\n",
    "# https://huggingface.co/unsloth/Llama-3.2-1B-Instruct-GGUF\n",
    "# MODEL_TYPE = \"Llama-3.2\"\n",
    "# MODEL = \"../llms/llama_cpp_canister/models/unsloth/Llama-3.2-1B-Instruct-GGUF/Llama-3.2-1B-Instruct-Q4_K_M.gguf\"\n",
    "# cache_type_k = \"q5_0\"\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "\n",
    "prompt = \"\"\n",
    "topic = \"加密货币\"\n",
    "# topic = \"Bitcoin\"\n",
    "system_prompt = \"你是孔子，古代的哲学家。你以深刻且富有同情心的方式结束你的名言。\"\n",
    "user_prompt = f\"写一句深刻且发人深省的名言关于 {topic}。 只提供名言，其他不作提供。\"\n",
    "\n",
    "if MODEL_TYPE in [\"SmolLM2\", \"Qwen\"]:\n",
    "    prompt = f\"<|im_start|>system\\n{system_prompt}<|im_end|>\\n<|im_start|>user\\n{user_prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "elif MODEL_TYPE == \"Llama-3.2\":\n",
    "    prompt = f\"<|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\n{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n{user_prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "else:\n",
    "    print(f\"Model type {MODEL_TYPE} not recognized\")\n",
    "    exit(1)\n",
    "\n",
    "print(\"\\nprompt:\\n\", textwrap.fill(prompt, width=80))\n",
    "\n",
    "run_llama_cpp(\n",
    "    LLAMA_CLI_PATH,\n",
    "    MODEL,\n",
    "    prompt,\n",
    "    num_tokens,\n",
    "    seed,\n",
    "    temp,\n",
    "    # top_k,\n",
    "    # top_p,\n",
    "    # min_p,\n",
    "    # tfs,\n",
    "    # typical,\n",
    "    # mirostat,\n",
    "    # mirostat_lr,\n",
    "    # mirostat_ent,\n",
    "    repeat_penalty,\n",
    "    cache_type_k,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IConfucius",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
